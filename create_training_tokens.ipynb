{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data is 18878847\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_data/train_data_Text.txt\", \"r\") as f:\n",
    "    train = f.read()\n",
    "with open(\"training_data/val_data_Text.txt\", \"r\") as f:\n",
    "    val = f.read()\n",
    "print(f\"Total amount of data is {len(train)+len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startofbook|><|title|>Assassin's Blade <|title|>Chapter 1 \n",
      "ren are resilient.\" \"They've lost their brother and their father, and now you're leaving them to be raised by an uncle they've seen only a few times -- \" \"They will come to know him; he is a good man.\n"
     ]
    }
   ],
   "source": [
    "print(train[:60])\n",
    "print(train[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex_bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tokenizer = regex_bpe_tokenizer.ApplyTokenizer(title=\"FantasyGPTv1\", tokenizer_folder_path=os.getcwd(),vocab_size=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer.encode(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[801, 802, 65, 481, 511, 259, 335, 399, 108, 573, 802, 67, 791, 375, 32, 49, 800, 83, 101, 727]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|startofbook|><|title|>Assassin's Blade<|title|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([801,802,65,481,511,259,335,399,108,573,802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compresion efficiency: 2.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Compresion efficiency: {round(len(train)/len(train_tokens),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_functions import HelperFunctionsClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HelperFunctionsClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_str = h.int_array_to_str(array=train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'801 802 65 481 511 259 335 399'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_str[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data/train_tokens_vocab800.txt\", \"w\") as f:\n",
    "    f.write(train_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens2 = h.convert_str_file_to_int_array(file_path=\"training_data/train_tokens_vocab800.txt\",convert_to_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_tokens == train_tokens2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = tokenizer.encode(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321, 104, 277, 309, 583, 282, 267, 374, 44, 518, 325, 334, 300, 414, 275, 382, 290, 757, 468, 44, 762, 718, 325, 422, 110, 346, 538, 504, 299, 262]\n",
      " This was some dream, or she had gone to Hell after all, because she couldn't exist in the\n"
     ]
    }
   ],
   "source": [
    "print(val_tokens[:30])\n",
    "print(tokenizer.decode(val_tokens[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens_str = h.int_array_to_str(array=val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data/val_tokens_vocab800.txt\", \"w\") as f:\n",
    "    f.write(val_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens2 = h.convert_str_file_to_int_array(file_path = \"training_data/val_tokens_vocab800.txt\",convert_to_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_tokens2 == val_tokens).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
