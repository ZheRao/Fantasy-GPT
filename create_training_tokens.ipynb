{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data is 18878847\n"
     ]
    }
   ],
   "source": [
    "with open(\"training_data/train_data_Text.txt\", \"r\") as f:\n",
    "    train = f.read()\n",
    "with open(\"training_data/val_data_Text.txt\", \"r\") as f:\n",
    "    val = f.read()\n",
    "print(f\"Total amount of data is {len(train)+len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startofbook|><|title|>Assassin's Blade <|title|>Chapter 1 \n",
      "ren are resilient.\" \"They've lost their brother and their father, and now you're leaving them to be raised by an uncle they've seen only a few times -- \" \"They will come to know him; he is a good man.\n"
     ]
    }
   ],
   "source": [
    "print(train[:60])\n",
    "print(train[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex_bpe_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tokenizer = regex_bpe_tokenizer.ApplyTokenizer(title=\"FantasyGPTv1\", tokenizer_folder_path=os.getcwd(),vocab_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer.encode(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2001, 2002, 65, 481, 511, 259, 335, 1088, 573, 2002, 67, 791, 375, 32, 49, 2000, 83, 101, 727, 299]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|title|>Assassin's Blade\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2002,65,481,511,259,335,1088,573])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compresion efficiency: 3.33\n"
     ]
    }
   ],
   "source": [
    "print(f\"Compresion efficiency: {round(len(train)/len(train_tokens),2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_functions import HelperFunctionsClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HelperFunctionsClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_str = h.int_array_to_str(array=train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2001 2002 65 481 511 259 335 1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens_str[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data/train_tokens.txt\", \"w\") as f:\n",
    "    f.write(train_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens2 = h.convert_str_file_to_int_array(file_path=\"training_data/train_tokens.txt\",convert_to_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_tokens == train_tokens2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = tokenizer.encode(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1636, 309, 583, 282, 928, 44, 518, 325, 334, 1054, 275, 382, 290, 757, 468, 44, 971, 325, 845, 346, 538, 504, 299, 262, 917, 684, 501, 334, 459, 1027]\n",
      " This was some dream, or she had gone to Hell after all, because she couldn't exist in the world where this had been done\n"
     ]
    }
   ],
   "source": [
    "print(val_tokens[:30])\n",
    "print(tokenizer.decode(val_tokens[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens_str = h.int_array_to_str(array=val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data/val_tokens.txt\", \"w\") as f:\n",
    "    f.write(val_tokens_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens2 = h.convert_str_file_to_int_array(file_path = \"training_data/val_tokens.txt\",convert_to_torch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val_tokens2 == val_tokens).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
