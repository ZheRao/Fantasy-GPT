{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of books is 18878845\n"
     ]
    }
   ],
   "source": [
    "with open(\"Cleaned Books/court_of_series-processed.txt\",\"r\") as f:\n",
    "    book1 = f.read()\n",
    "with open(\"Cleaned Books/the_dark_artifices-processed.txt\",\"r\") as f:\n",
    "    book2 = f.read()\n",
    "with open(\"Cleaned Books/the_mortal_instruments-processed.txt\",\"r\") as f:\n",
    "    book3 = f.read()\n",
    "with open(\"Cleaned Books/throne_of_glass-processed.txt\",\"r\") as f:\n",
    "    book4 = f.read()\n",
    "text = book1 + book2 + book3 + book4\n",
    "print(f\"Total length of books is {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex_bpe_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Run - 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = regex_bpe_tokenizer.TrainTokenizer(text=text,title=\"FantasyGPTv1\",special_token_list=['<|startofbook|>','<|title|>'],\n",
    "                                               final_vocab_size=800, fresh_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ab86aa3f164cc99063aa1c3bea3bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/544 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tokenizer2 = regex_bpe_tokenizer.ApplyTokenizer(title=\"FantasyGPTv1\",vocab_size=2000,tokenizer_folder_path=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' shru'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.vocab[1999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|startofchapter|>': 2000, '<|startofbook|>': 2001, '<|title|>': 2002}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2000: '<|startofchapter|>', 2001: '<|startofbook|>', 2002: '<|title|>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.inverse_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startofbook|><|title|>Court of Thrones and Roses <|title|>Chapter 1 <|startofchapter|>The forest h'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = text[:100]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2001,\n",
       " 2002,\n",
       " 67,\n",
       " 507,\n",
       " 116,\n",
       " 293,\n",
       " 821,\n",
       " 114,\n",
       " 1244,\n",
       " 291,\n",
       " 463,\n",
       " 605,\n",
       " 292,\n",
       " 2002,\n",
       " 67,\n",
       " 791,\n",
       " 375,\n",
       " 32,\n",
       " 49,\n",
       " 2000,\n",
       " 611,\n",
       " 1298,\n",
       " 329,\n",
       " 265]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer2.encode(text=text2)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startofbook|><|title|>Court of Thrones and Roses<|title|>Chapter 1<|startofchapter|>The forest h'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_decode = tokenizer2.decode(tokens=tokens)\n",
    "tokens_decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Run - 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = regex_bpe_tokenizer.TrainTokenizer(text=\" \",title=\"FantasyGPTv1\",special_token_list=['<|startofbook|>','<|title|>'],\n",
    "                                               final_vocab_size=2000, fresh_start=False, last_vocab_size=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startofchapter|>', '<|startofbook|>', '<|title|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eab4fac7c3d4896a774476b0b913144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Run - 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = regex_bpe_tokenizer.TrainTokenizer(text=\" \",title=\"FantasyGPTv1\",special_token_list=['<|startofbook|>','<|title|>'],\n",
    "                                               final_vocab_size=4000, fresh_start=False, last_vocab_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'ilt',\n",
       " b' true',\n",
       " b'aces',\n",
       " b' forg',\n",
       " b' perhaps',\n",
       " b'ze',\n",
       " b' street',\n",
       " b' Cour',\n",
       " b'Don',\n",
       " b' stro',\n",
       " b' green',\n",
       " b' warri',\n",
       " b' doesn',\n",
       " b'rys',\n",
       " b' miss',\n",
       " b' fig',\n",
       " b'iff',\n",
       " b'cket',\n",
       " b'oned',\n",
       " b' van',\n",
       " b' waiting',\n",
       " b'ape',\n",
       " b'ray',\n",
       " b'mor',\n",
       " b' ide',\n",
       " b'rie',\n",
       " b' How',\n",
       " b' wings',\n",
       " b' walls',\n",
       " b'ures',\n",
       " b' silver',\n",
       " b' coll',\n",
       " b'ines',\n",
       " b' bother',\n",
       " b' closed',\n",
       " b'aged',\n",
       " b'imm',\n",
       " b'dden',\n",
       " b' prom',\n",
       " b'uth',\n",
       " b'ane',\n",
       " b' swe',\n",
       " b' dagg',\n",
       " b' silence',\n",
       " b' par',\n",
       " b' cent',\n",
       " b' spoke',\n",
       " b'arm',\n",
       " b' course',\n",
       " b' mountain',\n",
       " b' fle',\n",
       " b'ult',\n",
       " b' wrist',\n",
       " b' touch',\n",
       " b' soldiers',\n",
       " b' lost',\n",
       " b'ches',\n",
       " b' army',\n",
       " b' du',\n",
       " b'ual',\n",
       " b' iron',\n",
       " b' doub',\n",
       " b' within',\n",
       " b'iding',\n",
       " b' chair',\n",
       " b'ots',\n",
       " b' pushed',\n",
       " b' pers',\n",
       " b' bur',\n",
       " b'iced',\n",
       " b'hemia',\n",
       " b'ks',\n",
       " b'ifted',\n",
       " b'vern',\n",
       " b' sign',\n",
       " b'ump',\n",
       " b' knees',\n",
       " b' Perhaps',\n",
       " b' shirt',\n",
       " b' contin',\n",
       " b'how',\n",
       " b' apart',\n",
       " b'but',\n",
       " b'ants',\n",
       " b' draw',\n",
       " b'ase',\n",
       " b' murm',\n",
       " b'hil',\n",
       " b'nding',\n",
       " b' list',\n",
       " b' drew',\n",
       " b' believe',\n",
       " b' break',\n",
       " b'owd',\n",
       " b' Be',\n",
       " b'ently',\n",
       " b' wrong',\n",
       " b' ear',\n",
       " b' demand',\n",
       " b' also',\n",
       " b' smell',\n",
       " b' watching',\n",
       " b' eye',\n",
       " b' scr',\n",
       " b' faerie',\n",
       " b' turn',\n",
       " b' Nehemia',\n",
       " b'room',\n",
       " b' gray',\n",
       " b' beautiful',\n",
       " b' gle',\n",
       " b' wound',\n",
       " b' cir',\n",
       " b' heav',\n",
       " b' question',\n",
       " b' Amren',\n",
       " b' ring',\n",
       " b' sitting',\n",
       " b'bered',\n",
       " b' loved',\n",
       " b'ces',\n",
       " b' Sil',\n",
       " b' Court',\n",
       " b'attered',\n",
       " b' stom',\n",
       " b'agged',\n",
       " b' answer',\n",
       " b'acing',\n",
       " b' Fen',\n",
       " b' soon',\n",
       " b' started',\n",
       " b' snapped',\n",
       " b' pie',\n",
       " b' idea',\n",
       " b' fine',\n",
       " b'olm',\n",
       " b' weapons',\n",
       " b' play',\n",
       " b' rock',\n",
       " b' reg',\n",
       " b' Every',\n",
       " b' dest',\n",
       " b'-f',\n",
       " b'aped',\n",
       " b'aves',\n",
       " b'ips',\n",
       " b' hit',\n",
       " b' pleas',\n",
       " b'no',\n",
       " b' feeling',\n",
       " b' appro',\n",
       " b'ging',\n",
       " b'we',\n",
       " b'pl',\n",
       " b'Yes',\n",
       " b' person',\n",
       " b' express',\n",
       " b' ph',\n",
       " b'ghed',\n",
       " b'This',\n",
       " b' finger',\n",
       " b' month',\n",
       " b'alcolm',\n",
       " b' straight',\n",
       " b' prob',\n",
       " b' hell',\n",
       " b' Valg',\n",
       " b' es',\n",
       " b'oud',\n",
       " b' hers',\n",
       " b' passed',\n",
       " b' sea',\n",
       " b'thorn',\n",
       " b' wide',\n",
       " b' pointed',\n",
       " b' running',\n",
       " b'cc',\n",
       " b' closer',\n",
       " b' res',\n",
       " b' isn',\n",
       " b'ates',\n",
       " b' sudden',\n",
       " b'ring',\n",
       " b'ara',\n",
       " b' best',\n",
       " b' inter',\n",
       " b' stairs',\n",
       " b' court',\n",
       " b' bring',\n",
       " b'aim',\n",
       " b' probably',\n",
       " b'ides',\n",
       " b' path',\n",
       " b' mess',\n",
       " b' Blackthorn',\n",
       " b' pat',\n",
       " b' fre',\n",
       " b' remembered',\n",
       " b' mark',\n",
       " b'elyn',\n",
       " b'rom',\n",
       " b' blin',\n",
       " b' One',\n",
       " b'arta',\n",
       " b' dang',\n",
       " b'ately',\n",
       " b' Dru',\n",
       " b' ble',\n",
       " b' swall',\n",
       " b' Down',\n",
       " b' doors',\n",
       " b' stru',\n",
       " b'arlan',\n",
       " b' speak',\n",
       " b' everyone',\n",
       " b'ushing',\n",
       " b' cor',\n",
       " b' die',\n",
       " b' grab',\n",
       " b'aked',\n",
       " b' scar',\n",
       " b' acc',\n",
       " b' Gav',\n",
       " b'ition',\n",
       " b'app',\n",
       " b' Fenrys',\n",
       " b' slam',\n",
       " b' trust',\n",
       " b' disc',\n",
       " b'artaq',\n",
       " b' breathed',\n",
       " b' certain',\n",
       " b' broken',\n",
       " b'world',\n",
       " b' fast',\n",
       " b' gleam',\n",
       " b' tem',\n",
       " b'selves',\n",
       " b' Adarlan',\n",
       " b' empt',\n",
       " b' desp',\n",
       " b' Ch',\n",
       " b' .',\n",
       " b' la',\n",
       " b' Helen',\n",
       " b' fur',\n",
       " b' pocket',\n",
       " b' sorry',\n",
       " b' shot',\n",
       " b'rawan',\n",
       " b' libr',\n",
       " b' stomach',\n",
       " b' Hy',\n",
       " b' less',\n",
       " b' Sartaq',\n",
       " b'ib',\n",
       " b' quietly',\n",
       " b'nded',\n",
       " b'iego',\n",
       " b' lead',\n",
       " b'-s',\n",
       " b'vers',\n",
       " b' je',\n",
       " b'My',\n",
       " b' creat',\n",
       " b'utes',\n",
       " b' hours',\n",
       " b'eries',\n",
       " b'ometimes',\n",
       " b' shout',\n",
       " b'anged',\n",
       " b' reve',\n",
       " b' friends',\n",
       " b\" '\",\n",
       " b'ocelyn',\n",
       " b' hope',\n",
       " b' inte',\n",
       " b' summ',\n",
       " b' Malcolm',\n",
       " b' longer',\n",
       " b' burned',\n",
       " b' followed',\n",
       " b' da',\n",
       " b' return',\n",
       " b' laughed',\n",
       " b' Rol',\n",
       " b' narrow',\n",
       " b'raid',\n",
       " b' making',\n",
       " b'actly',\n",
       " b'ern',\n",
       " b'bern',\n",
       " b' slowly',\n",
       " b' chin',\n",
       " b'rying',\n",
       " b'-c',\n",
       " b' castle',\n",
       " b' Terr',\n",
       " b' imag',\n",
       " b' shru')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemgetter(*range(1700,2000))(tokenizer2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
