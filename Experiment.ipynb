{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move necessary helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from update_utilities import update_utilities_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist in destination folder, it is now removed\n",
      "File copied, now the file is available to import from the destinated path\n",
      "File already exist in destination folder, it is now removed\n",
      "File copied, now the file is available to import from the destinated path\n",
      "File already exist in destination folder, it is now removed\n",
      "File copied, now the file is available to import from the destinated path\n",
      "File already exist in destination folder, it is now removed\n",
      "File copied, now the file is available to import from the destinated path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "update_utilities_class(file_name=\"Transformer.py\",current_path=os.getcwd()).run()\n",
    "update_utilities_class(file_name=\"custom_text_dataset.py\",current_path=os.getcwd()).run()\n",
    "update_utilities_class(file_name=\"loss_functions.py\",current_path=os.getcwd()).run()\n",
    "update_utilities_class(file_name=\"train_test_loop.py\",current_path=os.getcwd()).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_functions import HelperFunctionsClass\n",
    "h = HelperFunctionsClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = h.convert_str_file_to_int_array(file_path=\"training_data/train_tokens_vocab2000.txt\",convert_to_torch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = h.convert_str_file_to_int_array(file_path=\"training_data/val_tokens_vocab2000.txt\",convert_to_torch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5101623, 561744)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_text_dataset import slideTokenizedTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 400\n",
    "train_dataset = slideTokenizedTextDataset(full_txt=train_data, block_size=block_size)\n",
    "val_dataset = slideTokenizedTextDataset(full_txt=val_data, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5101223, 561344)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_num_samples = 600000\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=False,num_samples=train_num_samples)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,sampler=train_sampler,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num_samples = 100000\n",
    "val_sampler = torch.utils.data.RandomSampler(val_dataset,replacement=False,num_samples=val_num_samples)\n",
    "val_loader = DataLoader(val_dataset,sampler=val_sampler,batch_size=batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9375, 1562)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2003 # 2000 vocab size + 3 special tokens\n",
    "transformer = Transformer.TransformerClass(vocab_size=vocab_size,emb_dim=512,num_heads=8,n_layer=12,block_size=400, ff_multiplier=4,\n",
    "                                           dropout_rate_attention=0.1, dropout_rate_ff=0.2, dropout_rate_pos_enc=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Parameters 39.86 million\n"
     ]
    }
   ],
   "source": [
    "num_para = sum(p.numel() for p in transformer.parameters()) / 1e6\n",
    "print(f\"Total number of Parameters {round(num_para,2)} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_state_dict(torch.load(r\"baseline_vocab2000_12layer_400context stats\\baseline_vocab2000_12layer_400context weights\\baseline_vocab2000_12layer_400context_last_run4.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training stage 1\n",
    "**Rapid training with relatively higher lr, more validation check point, relatively lower number of validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test_loop import train_test_loop_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.AdamW(params=transformer.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=True\n",
    "num_epochs=1\n",
    "print_every = 855\n",
    "\n",
    "train_loop = train_test_loop_class(model=transformer,train_loader=train_loader,val_loader=val_loader,test_loader=None,\n",
    "                                   epochs=num_epochs, print_every_n_batch=print_every,device=device,\n",
    "                                   model_name=\"baseline_vocab2000_12layer_400context\",optimizer = optimizer, calculate_accuracy=False,\n",
    "                                   overwrite_message=overwrite,problem_type=\"Multiclass Classification\",\n",
    "                                   update_loss_fn=False, print_result=True, print_full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea31757512124269b36197ecdcf416ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 855 / 9375 || Average per-Batch Training Loss: 4.5761 || Average per-Batch Validation Loss: 4.0578\n",
      "Batch: 1710 / 9375 || Average per-Batch Training Loss: 3.8726 || Average per-Batch Validation Loss: 3.6186\n",
      "Batch: 2565 / 9375 || Average per-Batch Training Loss: 3.5018 || Average per-Batch Validation Loss: 3.3727\n",
      "Batch: 3420 / 9375 || Average per-Batch Training Loss: 3.2738 || Average per-Batch Validation Loss: 3.2481\n",
      "Batch: 4275 / 9375 || Average per-Batch Training Loss: 3.1178 || Average per-Batch Validation Loss: 3.1699\n",
      "Batch: 5130 / 9375 || Average per-Batch Training Loss: 2.9958 || Average per-Batch Validation Loss: 3.1205\n",
      "Batch: 5985 / 9375 || Average per-Batch Training Loss: 2.8905 || Average per-Batch Validation Loss: 3.1004\n",
      "Batch: 6840 / 9375 || Average per-Batch Training Loss: 2.7986 || Average per-Batch Validation Loss: 3.0873\n",
      "Batch: 7695 / 9375 || Average per-Batch Training Loss: 2.7124 || Average per-Batch Validation Loss: 3.0889\n",
      "Batch: 8550 / 9375 || Average per-Batch Training Loss: 2.6326 || Average per-Batch Validation Loss: 3.0978\n",
      "Batch: 9375 / 9375 || Average per-Batch Training Loss: 2.5583 || Average per-Batch Validation Loss: 3.1108\n",
      "\n",
      " All Done\n",
      "\n",
      "Overall training took 1.54 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop.overwrite_message = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10937, 1812)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_num_samples = 700000\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=False,num_samples=train_num_samples)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,sampler=train_sampler,drop_last=True)\n",
    "val_num_samples = 116000\n",
    "val_sampler = torch.utils.data.RandomSampler(val_dataset,replacement=False,num_samples=val_num_samples)\n",
    "val_loader = DataLoader(val_dataset,sampler=val_sampler,batch_size=batch_size,drop_last=True)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop.train_loader = train_loader\n",
    "train_loop.val_loader = val_loader\n",
    "train_loop.print_progress = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16abdfe8f6db4ae9b7b99031c27147af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1000 / 10937 || Average per-Batch Training Loss: 3.9907 || Average per-Batch Validation Loss: 3.9026\n",
      "Batch: 2000 / 10937 || Average per-Batch Training Loss: 3.9360 || Average per-Batch Validation Loss: 3.8435\n",
      "Batch: 3000 / 10937 || Average per-Batch Training Loss: 3.8818 || Average per-Batch Validation Loss: 3.7849\n",
      "Batch: 4000 / 10937 || Average per-Batch Training Loss: 3.8277 || Average per-Batch Validation Loss: 3.7312\n",
      "Batch: 5000 / 10937 || Average per-Batch Training Loss: 3.7759 || Average per-Batch Validation Loss: 3.6801\n",
      "Batch: 6000 / 10937 || Average per-Batch Training Loss: 3.7260 || Average per-Batch Validation Loss: 3.6347\n",
      "Batch: 7000 / 10937 || Average per-Batch Training Loss: 3.6819 || Average per-Batch Validation Loss: 3.5933\n",
      "Batch: 8000 / 10937 || Average per-Batch Training Loss: 3.6401 || Average per-Batch Validation Loss: 3.5548\n",
      "Batch: 9000 / 10937 || Average per-Batch Training Loss: 3.6003 || Average per-Batch Validation Loss: 3.5222\n",
      "Batch: 10000 / 10937 || Average per-Batch Training Loss: 3.5657 || Average per-Batch Validation Loss: 3.4918\n",
      "Batch: 10937 / 10937 || Average per-Batch Training Loss: 3.5339 || Average per-Batch Validation Loss: 3.4658\n",
      "\n",
      " All Done\n",
      "\n",
      "Overall training took 1.81 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Stage 2\n",
    "**slow fine-tuning with relatively lower lr, less validation check point, relatively higher number of validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "optimizer = torch.optim.AdamW(params=transformer.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 3125)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_num_samples = 800000\n",
    "train_sampler = torch.utils.data.RandomSampler(train_dataset,replacement=False,num_samples=train_num_samples)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,sampler=train_sampler,drop_last=True)\n",
    "val_num_samples = 200000\n",
    "val_sampler = torch.utils.data.RandomSampler(val_dataset,replacement=False,num_samples=val_num_samples)\n",
    "val_loader = DataLoader(val_dataset,sampler=val_sampler,batch_size=batch_size,drop_last=True)\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite=False\n",
    "num_epochs=1\n",
    "print_every = 2500\n",
    "\n",
    "train_loop = train_test_loop_class(model=transformer,train_loader=train_loader,val_loader=val_loader,test_loader=None,\n",
    "                                   epochs=num_epochs, print_every_n_batch=print_every,device=device,\n",
    "                                   model_name=\"baseline_vocab2000_12layer_400context\",optimizer = optimizer, calculate_accuracy=False,\n",
    "                                   overwrite_message=overwrite,problem_type=\"Multiclass Classification\",\n",
    "                                   update_loss_fn=False, print_result=True, print_full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29be6ad8e6f544cbbc4d645624f76a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2500 / 12500 || Average per-Batch Training Loss: 2.4663 || Average per-Batch Validation Loss: 3.0936\n",
      "Batch: 5000 / 12500 || Average per-Batch Training Loss: 2.4609 || Average per-Batch Validation Loss: 3.0943\n",
      "Batch: 7500 / 12500 || Average per-Batch Training Loss: 2.4551 || Average per-Batch Validation Loss: 3.0969\n",
      "Batch: 10000 / 12500 || Average per-Batch Training Loss: 2.4499 || Average per-Batch Validation Loss: 3.0988\n",
      "Batch: 12500 / 12500 || Average per-Batch Training Loss: 2.4443 || Average per-Batch Validation Loss: 3.1009\n",
      "\n",
      " All Done\n",
      "\n",
      "Overall training took 1.86 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop.overwrite_message = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-6\n",
    "optimizer = torch.optim.AdamW(params=transformer.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseline_vocab2000_2xcontextlen'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e3acbb43654c2e9f29a994d5e145e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2500 / 12500 || Average per-Batch Training Loss: 2.4386 || Average per-Batch Validation Loss: 3.1031\n",
      "Batch: 5000 / 12500 || Average per-Batch Training Loss: 2.4347 || Average per-Batch Validation Loss: 3.1060\n",
      "Batch: 7500 / 12500 || Average per-Batch Training Loss: 2.4284 || Average per-Batch Validation Loss: 3.1068\n",
      "Batch: 10000 / 12500 || Average per-Batch Training Loss: 2.4233 || Average per-Batch Validation Loss: 3.1087\n",
      "Batch: 12500 / 12500 || Average per-Batch Training Loss: 2.4186 || Average per-Batch Validation Loss: 3.1108\n",
      "\n",
      " All Done\n",
      "\n",
      "Overall training took 1.85 hours\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train_loop.model = transformer.to(device)\n",
    "train_loop.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "class generator:\n",
    "    def __init__(self, model, encoder, decoder, model_name):\n",
    "        self.model = model.cpu()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.model_name = model_name\n",
    "        self.folder_name = \"generated_text\"\n",
    "    \n",
    "    def _open_file(self):\n",
    "        if not os.path.exists(self.folder_name):\n",
    "            os.makedirs(self.folder_name)\n",
    "        self.f = open(os.path.join(self.folder_name,self.model_name)+\".txt\",\"a\")\n",
    "\n",
    "    \n",
    "    def generate_without_prompt(self, user_input, generation_length=300, block_size=512,default_start_token=True,pad_with=32):\n",
    "        self._open_file()\n",
    "        output_list = [user_input]\n",
    "        if default_start_token: user_input = '<|startofchapter|>' + user_input\n",
    "        # tokenize user input\n",
    "        input_tokens = torch.tensor(self.encoder(user_input),dtype=torch.long)\n",
    "        # pad tokens\n",
    "        if len(input_tokens) < block_size:\n",
    "            tokens = torch.full(size=(1,block_size),fill_value=pad_with,dtype=torch.long)\n",
    "            tokens[0,-len(input_tokens):] = input_tokens\n",
    "        else:\n",
    "            tokens = input_tokens[-block_size:].unsqueeze(0)\n",
    "        \n",
    "        m = f\"User input: {user_input}\\nGenerating----------------------------------------------------\\n\"\n",
    "        print(m)\n",
    "        self.f.write(\"\\n\\n\"+m+\"\\n\")\n",
    "\n",
    "        print_status = False\n",
    "        print_idx_start = 0\n",
    "        for i in range(generation_length):\n",
    "            if i % 30 == 1:\n",
    "                print_status = True\n",
    "            if (\".\" in output_list[-1] or \",\" in output_list[-1]) and print_status==True:\n",
    "                output_sequence = \"\".join(output_list[print_idx_start:])\n",
    "                print_idx_start = len(output_list)\n",
    "                print_status=False\n",
    "                print(output_sequence)\n",
    "                self.f.write(output_sequence+\"\\n\")\n",
    "            tokens_truncate = tokens[0,-block_size:]\n",
    "            logit = self.model(tokens_truncate)\n",
    "            logit = logit[0,-1,:]\n",
    "            prob = torch.nn.functional.softmax(logit,dim=0)\n",
    "            new_token = torch.multinomial(input=prob,num_samples=1)\n",
    "            tokens = torch.cat((tokens,new_token.unsqueeze(0)),dim=1)\n",
    "            output_list.append(self.decoder([new_token.item()]))\n",
    "        m = f\"\\nEnd of generation------------------------------------------------------------------\\n\"\n",
    "        print(m)\n",
    "        self.f.write(m+\"\\n\\n\")\n",
    "        self.f.close()\n",
    "        return output_list\n",
    "            \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from regex_bpe_tokenizer import ApplyTokenizer\n",
    "import os\n",
    "tokenizer = ApplyTokenizer(title=\"FantasyGPTv1\",vocab_size=2000,tokenizer_folder_path=os.getcwd())\n",
    "len(tokenizer.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator(transformer,tokenizer.encode,tokenizer.decode,\"baseline_vocab2000_12layer_400context_run5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: <|startofchapter|>There was an incident at the school, \n",
      "Generating----------------------------------------------------\n",
      "\n",
      "There was an incident at the school, utter or illusions that awoke me spelled on,\n",
      " bringing up my things. I only knew the cabin in the corner of my eye when preened to spend my time here and no one but me.\n",
      " My eyes burned with fragile as my fingers reached for the bed,\n",
      " some socket perched on the studio level on the floor. She pulled a small cup for it.\n",
      " \"Hidden marks from your tent,\" a growling nod. Anyone but the female kept screaming completely wanyverns poured out.\n",
      " \"They brought me in a prison. And I gave them more of a sadist.\"\n",
      " It wasn't his failure that had long since vanished from utter silence, that sight of her.\n",
      " Bile thing that smell ... on mighty, she couldn't read it as if it were possible to be clothed registering.\n",
      " The broken dish proximity of thread within her demeanor. No one could steal it, used all sense of hers.\n",
      " \"Actually, I think,\" a voice said from the sitting room ... Cairn. Had barely been able to spoil him.\n",
      "\n",
      "End of generation------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input=\"There was an incident at the school, \"\n",
    "output = gen.generate_without_prompt(user_input=user_input,generation_length=300,default_start_token=True,block_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
